import re
import string
from urllib.parse import urlparse, urljoin, urldefrag
from collections import Counter, deque
from bs4 import BeautifulSoup

# å…¨å±€å˜é‡
unique_pages = set()              # ä¿å­˜å·²è®¿é—®çš„ URLï¼ˆå»é™¤ fragmentï¼‰
page_count = 0                    # çˆ¬å–é¡µé¢æ€»æ•°
longest_page_word_count = 0       # å•è¯æ•°æœ€å¤šé¡µé¢çš„å•è¯æ•°
longest_page_url = ""             # å•è¯æ•°æœ€å¤šé¡µé¢çš„ URL
word_frequency = Counter()        # å…¨å±€å•è¯é¢‘ç‡ç»Ÿè®¡
ics_subdomains = {}               # ICS å­åŸŸé¡µé¢æ•°ï¼ˆä»…ç»Ÿè®¡å”¯ä¸€é¡µé¢ï¼‰

# ä¸ºé¿å… SimHash å†…å­˜é—®é¢˜ï¼Œåªä¿ç•™æœ€è¿‘ 1000 ä¸ªé¡µé¢çš„æŒ‡çº¹
page_simhashes = deque(maxlen=1000)

# é™åˆ¶ Wiki é¡µé¢ä¸å­åŸŸé¡µæ•°
max_wiki_pages = 200 
wiki_count = 0
max_pages_per_subdomain = 500   # æ¯ä¸ªå­åŸŸæœ€å¤šçˆ¬å– 500 é¡µ
subdomain_counts = {}           # è®¡æ•°æ¯ä¸ªå­åŸŸçš„çˆ¬å–é¡µæ•°
blocked_subdomains = set()      # è¶…è¿‡é™åˆ¶çš„å­åŸŸï¼Œå°†ç»ˆæ­¢åç»­è¯·æ±‚

# åœç”¨è¯ï¼ˆå¢åŠ äº†æ— æ„ä¹‰å¯¼èˆªè¯ï¼‰
STOP_WORDS = {
    "a", "an", "the", "and", "or", "in", "on", "at", "to", "of", "for", "by", 
    "is", "are", "was", "were", "this", "that", "it", "with", "as", "from",
    "up", "next", "prev", "size", "image", "original"
}

def is_valid_word(word):
    """è¿‡æ»¤å•è¯ï¼šå‰”é™¤å•å­—æ¯ã€çº¯æ•°å­—ä»¥åŠå…¨æ˜¯ç¬¦å·çš„å•è¯"""
    if len(word) == 1:
        return False
    if word.isdigit():
        return False  
    if re.match(r"^[^a-zA-Z0-9]+$", word):
        return False  
    return True

def is_low_value_page(url, text):
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºä½ä»·å€¼é¡µé¢ï¼š
    - Wiki é¡µé¢è¶…è¿‡é™åˆ¶æ•°é‡ï¼›
    - seminar-series é¡µé¢ï¼ˆåŒ…å«ç‰¹å®šå¹´ä»½æ ¼å¼ï¼‰è§†ä¸ºä½ä»·å€¼ã€‚
    """
    global wiki_count
    if "wiki.ics.uci.edu" in url:
        wiki_count += 1
        if wiki_count > max_wiki_pages:
            print(f"ğŸš€ Skipping excessive Wiki page: {url}")
            return True
    if "seminar-series" in url and re.search(r"20\d{2}-20\d{2}", url):
        print(f"ğŸš€ Skipping low-value seminar page: {url}")
        return True
    return False

def is_exceeding_subdomain_limit(url):
    """
    åˆ¤æ–­æ˜¯å¦è¶…è¿‡å•ä¸ªå­åŸŸæœ€å¤§çˆ¬å–é¡µæ•°ï¼š
    å¦‚æœè¯¥å­åŸŸå·²åœ¨ blocked_subdomains ä¸­ï¼Œåˆ™ç›´æ¥è¿”å› Trueï¼Œ
    å¦åˆ™ç´¯åŠ è®¡æ•°ï¼Œè¶…è¿‡é™åˆ¶åå°†å…¶åŠ å…¥ blocked_subdomains å¹¶è¿”å› Trueã€‚
    """
    global subdomain_counts, blocked_subdomains
    parsed = urlparse(url)
    subdomain = parsed.netloc
    if subdomain in blocked_subdomains:
        return True
    subdomain_counts[subdomain] = subdomain_counts.get(subdomain, 0) + 1
    if subdomain_counts[subdomain] > max_pages_per_subdomain:
        print(f"ğŸš€ Terminating crawl for subdomain {subdomain} (limit {max_pages_per_subdomain} reached)")
        blocked_subdomains.add(subdomain)
        return True
    return False

def is_large_page(text, url):
    """è·³è¿‡è¶…å¤§é¡µé¢ï¼ˆè¶…è¿‡ 10,000 è¯çš„é¡µé¢ï¼‰"""
    word_count = len(re.findall(r"\w+", text))
    if word_count > 10000:
        print(f"ğŸš€ Skipping large page: {word_count} words, {url}")
        return True
    return False

def is_valid(url):
    """
    æ£€æŸ¥ URL æ˜¯å¦ç¬¦åˆçˆ¬å–æ¡ä»¶ï¼š
    1. ç§»é™¤æ‰€æœ‰åŠ¨æ€å‚æ•°ï¼ˆåªä¿ç•™ scheme://netloc/pathï¼‰ã€‚
    2. åè®®å¿…é¡»ä¸º http/httpsã€‚
    3. åŸŸåå¿…é¡»å±äºå…è®¸èŒƒå›´ã€‚
    4. å±è”½å›¾ç‰‡åº“ã€ç›¸å†Œã€å¹»ç¯ç‰‡ç­‰é¡µé¢ã€‚
    5. è¿‡æ»¤é HTML æ–‡ä»¶ï¼ˆåŒ…æ‹¬ .txtã€.apkã€.zip ç­‰ï¼‰ã€‚
    """
    try:
        parsed = urlparse(url)
        # ç§»é™¤æ‰€æœ‰åŠ¨æ€å‚æ•°ï¼Œä¿ç•™ scheme://netloc/path
        normalized_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        parsed = urlparse(normalized_url)
        if parsed.scheme not in {"http", "https"}:
            return False

        allowed_domains = ("ics.uci.edu", "cs.uci.edu", "informatics.uci.edu", "stat.uci.edu")
        if not any(parsed.netloc.endswith(domain) for domain in allowed_domains):
            return False

        # å±è”½å›¾ç‰‡åº“ã€ç›¸å†Œã€å¹»ç¯ç‰‡ç­‰é¡µé¢
        gallery_keywords = ["gallery", "album", "slideshow"]
        if any(keyword in parsed.path.lower() for keyword in gallery_keywords):
            return False

        # è¿‡æ»¤é HTML æ–‡ä»¶ï¼ˆæ–°å¢ .txtã€.apk ç­‰ï¼‰
        if re.match(
            r".*\.(css|js|bmp|gif|jpe?g|ico"
            r"|png|tiff?|mid|mp2|mp3|mp4"
            r"|wav|avi|mov|mpeg|ram|m4v|mkv|ogg|ogv|pdf|txt|apk"
            r"|ps|eps|tex|ppt|pptx|doc|docx|xls|xlsx|names"
            r"|data|dat|exe|bz2|tar|msi|bin|7z|psd|dmg|iso"
            r"|epub|dll|cnf|tgz|sha1"
            r"|thmx|mso|arff|rtf|jar|csv"
            r"|rm|smil|wmv|swf|wma|zip|rar|gz|log)$",
            parsed.path.lower()
        ):
            return False

        return True
    except TypeError:
        print("TypeError for", url)
        return False

def extract_next_links(url, resp):
    """ä»é¡µé¢ä¸­æå–æ‰€æœ‰é“¾æ¥ï¼Œå¹¶è½¬æ¢ä¸ºç»å¯¹ URLï¼ˆåŒæ—¶å»é™¤é”šç‚¹ï¼‰"""
    extracted_links = []
    if resp.status != 200:
        return extracted_links
    try:
        html_content = resp.raw_response.content
    except AttributeError:
        return extracted_links

    soup = BeautifulSoup(html_content, "html.parser")
    for tag in soup.find_all("a"):
        href = tag.get("href")
        if href:
            absolute_url = urljoin(resp.url, href)
            defragged_url, _ = urldefrag(absolute_url)
            extracted_links.append(defragged_url)
    return extracted_links

def compute_simhash(text, hash_bits=64):
    """
    è®¡ç®—æ–‡æœ¬çš„ SimHash æŒ‡çº¹ï¼š
    1. ç®€å•åˆ†è¯ï¼Œå¹¶ä»…ä¿ç•™æœ‰æ•ˆå•è¯ã€‚
    2. å¯¹æ¯ä¸ªå•è¯è®¡ç®— hash å€¼ï¼Œç„¶åå¯¹æ¯ä¸€ä½ç´¯åŠ æƒå€¼ã€‚
    3. æ ¹æ®ç´¯åŠ ç»“æœæ„å»ºæŒ‡çº¹ï¼ˆæ­£åˆ™è®°ä¸º 1ï¼Œå¦åˆ™ä¸º 0ï¼‰ã€‚
    """
    tokens = re.findall(r'\w+', text.lower())
    tokens = [token for token in tokens if is_valid_word(token)]
    vector = [0] * hash_bits
    for token in tokens:
        token_hash = hash(token)
        for i in range(hash_bits):
            bitmask = 1 << i
            if token_hash & bitmask:
                vector[i] += 1
            else:
                vector[i] -= 1
    fingerprint = 0
    for i in range(hash_bits):
        if vector[i] > 0:
            fingerprint |= (1 << i)
    return fingerprint

def hamming_distance(hash1, hash2):
    """è®¡ç®—ä¸¤ä¸ª SimHash æŒ‡çº¹ä¹‹é—´çš„ Hamming è·ç¦»"""
    x = hash1 ^ hash2
    distance = 0
    while x:
        distance += 1
        x &= x - 1
    return distance

def scraper(url, resp):
    """
    ä¸»çˆ¬è™«å‡½æ•°ï¼š
    1. URL å»é‡ã€æ›´æ–°ç»Ÿè®¡æ•°æ®ï¼›
    2. è§£æé¡µé¢æ–‡æœ¬ï¼Œè¿‡æ»¤ä½ä»·å€¼ã€è¶…å¤§ã€è¿‘é‡å¤ä»¥åŠå†…å®¹å¤ªå°‘çš„é¡µé¢ï¼›
    3. æ›´æ–°å•è¯é¢‘ç‡ã€æœ€é•¿é¡µé¢åŠ ICS å­åŸŸç»Ÿè®¡ï¼›
    4. è¿”å›é¡µé¢ä¸­ç¬¦åˆæ¡ä»¶çš„é“¾æ¥ã€‚
    """
    global unique_pages, page_count, longest_page_word_count, longest_page_url
    global word_frequency, ics_subdomains, page_simhashes

    url_no_fragment, _ = urldefrag(url)
    if url_no_fragment in unique_pages:
        return []
    unique_pages.add(url_no_fragment)
    page_count += 1

    if resp.status == 200 and hasattr(resp, "raw_response") and resp.raw_response:
        try:
            html_content = resp.raw_response.content
            soup = BeautifulSoup(html_content, "html.parser")
            text = soup.get_text(separator=" ", strip=True)

            # è·³è¿‡ä½ä»·å€¼é¡µé¢ä¸è¶…å¤§é¡µé¢
            if is_low_value_page(url_no_fragment, text):
                return []
            if is_large_page(text, url_no_fragment):
                return []

            # ç½‘é¡µæŒ‡çº¹æ£€æµ‹ï¼Œé¿å…ç²¾ç¡®æˆ–è¿‘ä¼¼é‡å¤é¡µé¢
            simhash_val = compute_simhash(text)
            for existing_simhash in page_simhashes:
                if hamming_distance(simhash_val, existing_simhash) <= 3:
                    print(f"ğŸš€ Skipping near duplicate page: {url_no_fragment}")
                    return []
            page_simhashes.append(simhash_val)

            if is_exceeding_subdomain_limit(url_no_fragment):
                return []

            # ç»Ÿè®¡æœ‰æ•ˆå•è¯æ•°ï¼ˆè¿‡æ»¤åœç”¨è¯å’Œæ— æ•ˆå•è¯ï¼‰
            words = re.findall(r"\w+", text.lower())
            filtered_words = [w for w in words if w not in STOP_WORDS and is_valid_word(w)]
            # å¦‚æœæœ‰æ•ˆå•è¯æ•°è¿‡å°‘ï¼Œåˆ™è®¤ä¸ºé¡µé¢æ— å®è´¨å†…å®¹
            if len(filtered_words) < 50:
                print(f"ğŸš€ Skipping low-content page: {url_no_fragment}")
                return []

            # å¯é€‰ï¼šå¦‚æ£€æµ‹é¡µé¢ä¸­å¯¼èˆªè¯å æ¯”è¾ƒé«˜ï¼Œåˆ™è·³è¿‡ï¼ˆå¯¼èˆªé¡µé¢ï¼‰
            NAV_TERMS = {"up", "next", "prev", "size", "image", "original"}
            if filtered_words:
                nav_count = sum(1 for w in filtered_words if w in NAV_TERMS)
                if (nav_count / len(filtered_words)) > 0.1:
                    print(f"ğŸš€ Skipping navigation heavy page: {url_no_fragment}")
                    return []

            current_word_count = len(filtered_words)
            if current_word_count > longest_page_word_count:
                longest_page_word_count = current_word_count
                longest_page_url = url_no_fragment

            word_frequency.update(filtered_words)

            parsed = urlparse(url_no_fragment)
            if parsed.netloc.endswith("ics.uci.edu"):
                ics_subdomains[parsed.netloc] = ics_subdomains.get(parsed.netloc, 0) + 1

        except Exception as e:
            print(f"Error processing page {url_no_fragment}: {e}")

    links = extract_next_links(url, resp)
    return [link for link in links if is_valid(link)]
    
def save_report():
    with open("crawl_report.txt", "w", encoding="utf-8") as f:
        f.write("=== Crawler Statistics ===\n")
        f.write(f"Total unique pages: {len(unique_pages)}\n")
        f.write(f"Longest page: {longest_page_url}\n")
        f.write(f"Word count in longest page: {longest_page_word_count}\n\n")
        f.write("Top 50 most common words (excluding stop words):\n")
        for word, count in word_frequency.most_common(50):
            f.write(f"{word}: {count}\n")
        f.write("\nICS Subdomains (sorted alphabetically):\n")
        for subdomain, count in sorted(ics_subdomains.items()):
            f.write(f"{subdomain}: {count}\n")
