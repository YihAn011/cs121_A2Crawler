import re
from urllib.parse import urlparse, urljoin, urldefrag, parse_qs, unquote
from bs4 import BeautifulSoup
from collections import Counter
import atexit
import hashlib

# SimHash calculation function
# SimHash计算函数
# Purpose: Generate a 64-bit hash (SimHash) for a given text to identify near-duplicate content.
# 作用：为给定文本生成64位的哈希值（SimHash），用于识别近似重复的内容。
def simhash(text, hBits=64):
    """
    Calculate SimHash for the given text.
    计算给定文本的SimHash值。

    Steps:
    步骤：
    1. Split text into words and count the frequency of each word.
    1. 将文本拆分为单词，并计算每个单词的频率。
    2. Hash each word using MD5 and convert it into a 64-bit integer.
    2. 使用MD5对每个单词进行哈希，并将其转换为64位整数。
    3. Create a hash vector where each bit position is adjusted based on word frequency.
    3. 创建哈希向量，并根据单词频率调整每个位的位置。
    4. Generate the final SimHash by setting bits to 1 if the corresponding vector value is positive.
    4. 如果对应向量值为正，则将该位设置为1，生成最终的SimHash。

    Args:
    参数：
        text (str): Input text for hash calculation.
        text (str)：用于哈希计算的输入文本。
        hBits (int): Number of bits for the hash, default is 64.
        hBits (int)：哈希的位数，默认为64。

    Returns:
    返回：
        int: The calculated SimHash value.
        int：计算得到的SimHash值。
    """
    words = text.split()
    weights = Counter(words)  # Count word frequency 统计单词频率

    hVec = [0] * hBits  # Initialize hash vector 初始化哈希向量

    for word, weight in weights.items():
        hashValue = int(hashlib.md5(word.encode()).hexdigest(), 16) & ((1 << hBits) - 1)
        for i in range(hBits):
            bit = 1 if (hashValue & (1 << i)) else -1
            hVec[i] += bit * weight

    simValue = 0
    for i in range(hBits):
        if hVec[i] > 0:
            simValue |= (1 << i)

    return simValue


# Hamming Distance calculation function
# 汉明距离计算函数
# Purpose: Calculate the number of differing bits between two SimHash values.
# 作用：计算两个SimHash值之间不同位的数量。
def hamDis(hash1, hash2):
    """
    Calculate the Hamming distance between two SimHash values.
    计算两个SimHash值之间的汉明距离。

    Args:
    参数：
        hash1 (int): First SimHash value.
        hash1 (int)：第一个SimHash值。
        hash2 (int): Second SimHash value.
        hash2 (int)：第二个SimHash值。

    Returns:
    返回：
        int: The Hamming distance (number of differing bits).
        int：汉明距离（不同位的数量）。
    """
    return bin(hash1 ^ hash2).count('1')


# Normalize URL by removing fragments
# 通过移除片段来规范化URL
# Purpose: Ensure consistent URL comparison by stripping fragments ("#" part).
# 作用：通过移除片段（“#”部分）来保证URL比较的一致性。
def normalize_url(url):
    """
    Normalize the URL by removing fragments.
    通过移除片段来规范化URL。

    Args:
    参数：
        url (str): Input URL.
        url (str)：输入的URL。

    Returns:
    返回：
        str: Normalized URL without fragments.
        str：去除片段后的规范化URL。
    """
    return urldefrag(url)[0]


# Global variables for tracking crawling state
# 用于跟踪爬取状态的全局变量
alredySeenURL = set()  # Set to store unique URLs 存储唯一URL的集合
wordCount = Counter()  # Counter to track word frequency 统计单词频率的计数器
subdUnique = {}        # Dictionary to store unique pages per subdomain 存储每个子域名唯一页面的字典
subC = {}              # Subdomain page count 子域页面计数
longestPage = {"url": None, "word_count": 0}  # Longest page tracker 记录最长页面
STOP_WORDS = {
    "the", "and", "a", "an", "to", "is", "in", "that", "it", "on", "for", "with",
    "as", "was", "at", "by", "this", "from", "or", "be", "are", "of", "not", "but",
    "we", "can", "if", "so", "about", "all", "one", "you", "your", "which",
    "have", "has", "they", "their", "there", "some", "my", "our", "more", "will",
    "would", "should", "could"
}
simCache = {}  # Cache to store SimHash values of visited pages 存储已访问页面SimHash值的缓存


# Main scraper function
# 主要的爬虫函数
# Purpose: Process a given URL, extract links, detect duplicates, and count words.
# 作用：处理给定的URL，提取链接，检测重复，并统计单词。
def scraper(url, resp):
    """
    Scrape the given URL, extract links, and analyze content.
    爬取给定的URL，提取链接，并分析内容。

    Steps:
    步骤：
    1. Check the response status and content type.
    1. 检查响应状态和内容类型。
    2. Skip previously visited or invalid URLs.
    2. 跳过已访问或无效的URL。
    3. Calculate SimHash and skip near-duplicate pages.
    3. 计算SimHash并跳过近似重复的页面。
    4. Extract hyperlinks and count words.
    4. 提取超链接并统计单词。
    5. Update statistics for word count, subdomains, and longest page.
    5. 更新单词统计、子域名和最长页面的统计信息。

    Args:
    参数：
        url (str): The URL being scraped.
        url (str)：正在爬取的URL。
        resp (Response): The response object containing page content.
        resp (Response)：包含页面内容的响应对象。

    Returns:
    返回：
        list: List of valid links extracted from the page.
        list：从页面中提取的有效链接列表。
    """
    global alredySeenURL, wordCount, subC, longestPage, subdUnique

    # Skip invalid response codes 跳过无效的响应代码
    if resp.status in [404, 608, 403, 500]:
        print(f"Skipping due to error {resp.status}: {url}")
        return []

    normalized_url = normalize_url(url)
    if normalized_url in alredySeenURL:
        print(f"Already seen: {normalized_url}")
        return []

    if resp.raw_response is None:
        print(f"No raw response: {url}")
        return []

    # Content type filtering (skip non-HTML content)
    # 内容类型过滤（跳过非HTML内容）
    content_type = resp.raw_response.headers.get("Content-Type", "").lower()
    blocked_types = [
        "application/pdf", "application/msword", "audio/mpeg", "video/mp4",
        "image/png", "image/jpeg", "application/zip", "application/x-gzip"
    ]
    if any(ext in content_type for ext in blocked_types):
        print(f"Skipping blocked content type ({content_type}): {url}")
        return []

    # Mark URL as seen 将URL标记为已访问
    alredySeenURL.add(normalized_url)
    print(f"Total unique pages seen: {len(alredySeenURL)}")

    # Track subdomain occurrences 统计子域名出现次数
    parsed = urlparse(url)
    if parsed.netloc.endswith(".ics.uci.edu"):
        subdUnique.setdefault(parsed.netloc, set()).add(normalized_url)

    # Extract links and words 提取链接和单词
    links, word_count = extract_next_links(url, resp)
    text = " ".join(word_count)
    simForPage = simhash(text)

    # Check for near-duplicate pages using SimHash
    # 使用SimHash检查近似重复的页面
    for old_url, old_hash in simCache.items():
        if hamDis(simForPage, old_hash) < 5:
            print(f"Duplicate detected: {url} and {old_url}")
            return []

    # Store SimHash for future comparisons
    # 存储SimHash以供未来比较
    simCache[url] = simForPage

    # Update word count and longest page
    # 更新单词计数和最长页面
    wordCount.update(word_count)
    if word_count and len(word_count) > longestPage["word_count"]:
        longestPage.update({"url": url, "word_count": len(word_count)})

    # Update subdomain count
    # 更新子域名计数
    if "ics.uci.edu" in parsed.netloc:
        subC[parsed.netloc] = subC.get(parsed.netloc, 0) + 1

    return [link for link in links if is_valid(link)]


# Link and word extraction function
# 链接和单词提取函数
# Purpose: Extract all hyperlinks and non-stop words from a webpage.
# 作用：从网页中提取所有超链接和非停用词。
def extract_next_links(url, resp):
    """
    Extract hyperlinks and words from the page content.
    从页面内容中提取超链接和单词。

    Args:
    参数：
        url (str): The URL being processed.
        url (str)：正在处理的URL。
        resp (Response): The response object containing page content.
        resp (Response)：包含页面内容的响应对象。

    Returns:
    返回：
        tuple: A list of extracted links and a list of non-stop words.
        tuple：提取的链接列表和非停用词列表。
    """
