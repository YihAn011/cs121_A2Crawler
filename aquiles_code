import re
from urllib.parse import urlparse, urljoin, urldefrag
from collections import Counter
from bs4 import BeautifulSoup
import string

unique_pages = set()              
page_count = 0                    
longest_page_word_count = 0      
longest_page_url = ""             
word_frequency = Counter()        
ics_subdomains = {}              

STOP_WORDS = {
    "a", "an", "the", "and", "or", "in", "on", "at", "to", "of", "for", "by", 
    "is", "are", "was", "were", "this", "that", "it", "with", "as", "from"
}
def is_valid_word(word):

    if len(word) == 1 and word not in string.ascii_lowercase:
        return False 
    if word.isdigit():
        return False  
    if re.match(r"^[^a-zA-Z0-9]+$", word):
        return False  
    return True
def scraper(url, resp):

    global unique_pages, page_count, longest_page_word_count, longest_page_url, word_frequency, ics_subdomains

    url_no_fragment, _ = urldefrag(url)
    if url_no_fragment not in unique_pages:
        unique_pages.add(url_no_fragment)
        page_count += 1

    if resp.status == 200 and hasattr(resp, "raw_response") and resp.raw_response:
        try:
            html_content = resp.raw_response.content
            soup = BeautifulSoup(html_content, "html.parser")
            text = soup.get_text(separator=" ", strip=True)
            words = re.findall(r"\w+", text.lower())
            filtered_words = [w for w in words if w not in STOP_WORDS and is_valid_word(w)]
            current_word_count = len(filtered_words)

            if current_word_count > longest_page_word_count:
                longest_page_word_count = current_word_count
                longest_page_url = url_no_fragment

            word_frequency.update(filtered_words)

            parsed = urlparse(url_no_fragment)
            if parsed.netloc.endswith("ics.uci.edu"):
                ics_subdomains[parsed.netloc] = ics_subdomains.get(parsed.netloc, 0) + 1

        except Exception as e:
            print("Error processing page {}: {}".format(url_no_fragment, e))

    links = extract_next_links(url, resp)
    return [link for link in links if is_valid(link)]

def extract_next_links(url, resp):
    extracted_links = []

    if resp.status != 200:
        return extracted_links

    try:
        html_content = resp.raw_response.content
    except AttributeError:
        return extracted_links

    soup = BeautifulSoup(html_content, "html.parser")
    for tag in soup.find_all("a"):
        href = tag.get("href")
        if href:
            absolute_url = urljoin(resp.url, href)
            defragged_url, _ = urldefrag(absolute_url)
            extracted_links.append(defragged_url)

    return extracted_links

def is_valid(url):
    try:
        parsed = urlparse(url)
        if parsed.scheme not in {"http", "https"}:
            return False

        allowed_domains = ("ics.uci.edu", "cs.uci.edu", "informatics.uci.edu", "stat.uci.edu")
        if not any(parsed.netloc.endswith(domain) for domain in allowed_domains):
            return False

        return not re.match(
            r".*\.(css|js|bmp|gif|jpe?g|ico"
            r"|png|tiff?|mid|mp2|mp3|mp4"
            r"|wav|avi|mov|mpeg|ram|m4v|mkv|ogg|ogv|pdf"
            r"|ps|eps|tex|ppt|pptx|doc|docx|xls|xlsx|names"
            r"|data|dat|exe|bz2|tar|msi|bin|7z|psd|dmg|iso"
            r"|epub|dll|cnf|tgz|sha1"
            r"|thmx|mso|arff|rtf|jar|csv"
            r"|rm|smil|wmv|swf|wma|zip|rar|gz)$", parsed.path.lower())
    except TypeError:
        print("TypeError for", url)
        raise

def save_report():

    with open("crawl_report.txt", "w", encoding="utf-8") as f:
        f.write("=== Crawler Statistics ===\n")
        f.write(f"Total unique pages: {len(unique_pages)}\n")
        f.write(f"Longest page: {longest_page_url}\n")
        f.write(f"Word count in longest page: {longest_page_word_count}\n\n")

        f.write("Top 50 most common words (excluding stop words):\n")
        for word, count in word_frequency.most_common(50):
            f.write(f"{word}: {count}\n")

        f.write("\nICS Subdomains (sorted alphabetically):\n")
        for subdomain, count in sorted(ics_subdomains.items()):
            f.write(f"{subdomain}: {count}\n")
