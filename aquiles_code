import re
import string
from urllib.parse import urlparse, urljoin, urldefrag
from collections import Counter
from bs4 import BeautifulSoup


unique_pages = set()              
page_count = 0                    
longest_page_word_count = 0       
longest_page_url = ""            
word_frequency = Counter()       
ics_subdomains = {}           
page_simhashes = []             


max_wiki_pages = 200 
wiki_count = 0
max_pages_per_subdomain = 10000  
subdomain_counts = {}           

# 停用词
STOP_WORDS = {
    "a", "an", "the", "and", "or", "in", "on", "at", "to", "of", "for", "by", 
    "is", "are", "was", "were", "this", "that", "it", "with", "as", "from"
}

def is_valid_word(word):

    if len(word) == 1:
        return False
    if word.isdigit():
        return False  
    if re.match(r"^[^a-zA-Z0-9]+$", word):
        return False  
    return True

def is_low_value_page(url, text):

    global wiki_count
    if "wiki.ics.uci.edu" in url:
        wiki_count += 1
        if wiki_count > max_wiki_pages:
            print(f"🚀 Skipping excessive Wiki page: {url}")
            return True
    if "seminar-series" in url and re.search(r"20\d{2}-20\d{2}", url):
        print(f"🚀 Skipping low-value seminar page: {url}")
        return True
    return False

def is_exceeding_subdomain_limit(url):
    global subdomain_counts
    parsed = urlparse(url)
    subdomain = parsed.netloc
    subdomain_counts[subdomain] = subdomain_counts.get(subdomain, 0) + 1
    if subdomain_counts[subdomain] > max_pages_per_subdomain:
        print(f"🚀 Skipping {subdomain}, exceeded {max_pages_per_subdomain} pages")
        return True
    return False

def is_large_page(text, url):
    word_count = len(re.findall(r"\w+", text))
    if word_count > 10000:
        print(f"🚀 Skipping large page: {word_count} words, {url}")
        return True
    return False

def is_valid(url):
    try:
        parsed = urlparse(url)
        if parsed.scheme not in {"http", "https"}:
            return False

        allowed_domains = ("ics.uci.edu", "cs.uci.edu", "informatics.uci.edu", "stat.uci.edu")
        if not any(parsed.netloc.endswith(domain) for domain in allowed_domains):
            return False

        if re.match(
            r".*\.(css|js|bmp|gif|jpe?g|ico"
            r"|png|tiff?|mid|mp2|mp3|mp4"
            r"|wav|avi|mov|mpeg|ram|m4v|mkv|ogg|ogv|pdf"
            r"|ps|eps|tex|ppt|pptx|doc|docx|xls|xlsx|names"
            r"|data|dat|exe|bz2|tar|msi|bin|7z|psd|dmg|iso"
            r"|epub|dll|cnf|tgz|sha1"
            r"|thmx|mso|arff|rtf|jar|csv"
            r"|rm|smil|wmv|swf|wma|zip|rar|gz|log)$",
            parsed.path.lower()
        ):
            return False

        return True
    except TypeError:
        print("TypeError for", url)
        return False

def extract_next_links(url, resp):
    extracted_links = []
    if resp.status != 200:
        return extracted_links
    try:
        html_content = resp.raw_response.content
    except AttributeError:
        return extracted_links

    soup = BeautifulSoup(html_content, "html.parser")
    for tag in soup.find_all("a"):
        href = tag.get("href")
        if href:
            absolute_url = urljoin(resp.url, href)
            defragged_url, _ = urldefrag(absolute_url)
            extracted_links.append(defragged_url)
    return extracted_links

def compute_simhash(text, hash_bits=64):

    tokens = re.findall(r'\w+', text.lower())
    tokens = [token for token in tokens if is_valid_word(token)]
    vector = [0] * hash_bits
    for token in tokens:
        token_hash = hash(token)
        for i in range(hash_bits):
            bitmask = 1 << i
            if token_hash & bitmask:
                vector[i] += 1
            else:
                vector[i] -= 1
    fingerprint = 0
    for i in range(hash_bits):
        if vector[i] > 0:
            fingerprint |= (1 << i)
    return fingerprint

def hamming_distance(hash1, hash2):
    x = hash1 ^ hash2
    distance = 0
    while x:
        distance += 1
        x &= x - 1
    return distance

def scraper(url, resp):
    global unique_pages, page_count, longest_page_word_count, longest_page_url, word_frequency, ics_subdomains, page_simhashes

    url_no_fragment, _ = urldefrag(url)
    if url_no_fragment in unique_pages:
        return []
    unique_pages.add(url_no_fragment)
    page_count += 1

    if resp.status == 200 and hasattr(resp, "raw_response") and resp.raw_response:
        try:
            html_content = resp.raw_response.content
            soup = BeautifulSoup(html_content, "html.parser")
            text = soup.get_text(separator=" ", strip=True)

            if is_low_value_page(url_no_fragment, text):
                return []

            if is_large_page(text, url_no_fragment):
                return []

            simhash_val = compute_simhash(text)
            for existing_simhash in page_simhashes:
                if hamming_distance(simhash_val, existing_simhash) <= 3:
                    print(f"🚀 Skipping near duplicate page: {url_no_fragment}")
                    return []
            page_simhashes.append(simhash_val)

            if is_exceeding_subdomain_limit(url_no_fragment):
                return []

            words = re.findall(r"\w+", text.lower())
            filtered_words = [w for w in words if w not in STOP_WORDS and is_valid_word(w)]
            current_word_count = len(filtered_words)
            if current_word_count > longest_page_word_count:
                longest_page_word_count = current_word_count
                longest_page_url = url_no_fragment

            word_frequency.update(filtered_words)

            parsed = urlparse(url_no_fragment)
            if parsed.netloc.endswith("ics.uci.edu"):
                ics_subdomains[parsed.netloc] = ics_subdomains.get(parsed.netloc, 0) + 1

        except Exception as e:
            print(f"Error processing page {url_no_fragment}: {e}")

    links = extract_next_links(url, resp)
    return [link for link in links if is_valid(link)]

def save_report():
    with open("crawl_report.txt", "w", encoding="utf-8") as f:
        f.write("=== Crawler Statistics ===\n")
        f.write(f"Total unique pages: {len(unique_pages)}\n")
        f.write(f"Longest page: {longest_page_url}\n")
        f.write(f"Word count in longest page: {longest_page_word_count}\n\n")
        f.write("Top 50 most common words (excluding stop words):\n")
        for word, count in word_frequency.most_common(50):
            f.write(f"{word}: {count}\n")
        f.write("\nICS Subdomains (sorted alphabetically):\n")
        for subdomain, count in sorted(ics_subdomains.items()):
            f.write(f"{subdomain}: {count}\n")
